<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width,initial-scale=1"/>
  <title>Interview ‚Äî AI Interview Trainer</title>
  <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
</head>
<body class="interview-page">
  <div class="page-card wide">
    <h2>Live Interview ‚Äî Simulation</h2>

    <div class="interview-layout">
      <!-- Left: Video -->
      <div class="video-pane">
        <div class="video-wrap">
          <video id="videoElem" playsinline autoplay muted></video>
        </div>
        <div style="margin-top:12px;">
          <button id="enableMedia" class="btn">Enable Camera & Mic</button>
          <button id="stopMedia" class="btn outline" disabled>Stop</button>
          <div id="mediaStatus" class="muted" style="margin-top:8px">Camera is off ‚Äî questions will appear after enabling.</div>
        </div>
      </div>

      <!-- Right: Questions & Controls -->
      <div class="control-pane">
        <div id="qPanel" class="hidden">
          <div class="question-box" id="questionText">Press Start Interview to begin</div>

          <div class="controls-row">
            <button id="startInterview" class="btn">Start Interview</button>
            <button id="speakQ" class="btn" disabled>üîà Speak Question</button>
            <button id="recordBtn" class="btn" style="background:#1db954" disabled>‚óè Record</button>
            <button id="stopRec" class="btn outline" disabled>‚ñ† Stop</button>
            <button id="nextQ" class="btn outline" disabled>Next</button>
          </div>

          <div style="margin-top:12px">
            <div class="muted">Question <span id="qIndex">0</span> / <span id="qTotal">0</span></div>
            <div class="progress"><div id="progressBar"></div></div>
          </div>

          <div style="margin-top:12px">
            <h3>Latest Transcript</h3>
            <div id="transcript" class="transcript">‚Äî</div>
            <h3 style="margin-top:12px">Recordings</h3>
            <div id="recordingsList" class="recordings-list">none yet</div>
          </div>

          <div id="endControls" style="margin-top:14px"></div>
        </div>

        <div id="mediaBlockedPanel" class="muted">
          Questions appear after you enable camera & microphone.
        </div>
      </div>
    </div>
  </div>

<script>
// client-side interaction
const questions = {{ questions|tojson }};
let qIdx = 0;
let recordings = [];
let mediaStream = null;
let mediaRecorder = null;
let recordedChunks = [];
let recognition = null;
let isRecording = false;

// elements
const videoElem = document.getElementById('videoElem');
const enableMedia = document.getElementById('enableMedia');
const stopMedia = document.getElementById('stopMedia');
const mediaStatus = document.getElementById('mediaStatus');
const qPanel = document.getElementById('qPanel');
const startInterview = document.getElementById('startInterview');
const speakQ = document.getElementById('speakQ');
const recordBtn = document.getElementById('recordBtn');
const stopRec = document.getElementById('stopRec');
const nextQ = document.getElementById('nextQ');
const questionText = document.getElementById('questionText');
const qIndex = document.getElementById('qIndex');
const qTotal = document.getElementById('qTotal');
const progressBar = document.getElementById('progressBar');
const transcriptEl = document.getElementById('transcript');
const recordingsList = document.getElementById('recordingsList');
const endControls = document.getElementById('endControls');

qTotal.innerText = questions.length || 0;

// enable camera & mic - until user allows, questions hidden
enableMedia.onclick = async () => {
  try {
    mediaStream = await navigator.mediaDevices.getUserMedia({ video: { width:640 }, audio: true });
    videoElem.srcObject = mediaStream;
    mediaStatus.innerText = "Camera & mic enabled.";
    enableMedia.disabled = true;
    stopMedia.disabled = false;
    // show questions panel now that media is available
    qPanel.classList.remove('hidden');
    document.getElementById('mediaBlockedPanel').style.display = 'none';
  } catch (e) {
    alert("Unable to access camera/microphone. Please allow permissions.");
  }
};

stopMedia.onclick = () => {
  if(mediaStream){
    mediaStream.getTracks().forEach(t => t.stop());
    mediaStream = null;
    videoElem.srcObject = null;
  }
  mediaStatus.innerText = "Camera stopped.";
  enableMedia.disabled = false;
  stopMedia.disabled = true;
  qPanel.classList.add('hidden');
  document.getElementById('mediaBlockedPanel').style.display = 'block';
};

// show question
function showQuestion(i){
  if(i<0 || i>=questions.length) return;
  questionText.innerText = questions[i];
  qIndex.innerText = i+1;
  progressBar.style.width = `${Math.round(((i+1)/questions.length)*100)}%`;
}

// start interview
startInterview.onclick = () => {
  if(questions.length === 0){ alert("No questions configured."); return; }
  qIdx = 0;
  showQuestion(qIdx);
  speakQ.disabled = false;
  recordBtn.disabled = false;
  nextQ.disabled = false;
  startInterview.disabled = true;
};

// speak question
speakQ.onclick = () => {
  const u = new SpeechSynthesisUtterance(questions[qIdx] || "No question.");
  speechSynthesis.speak(u);
};

// prepare MediaRecorder for audio (microphone)
async function prepareRecorder(){
  if(!mediaStream){
    // fallback: ask audio only
    try {
      mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    } catch(e){
      alert("Microphone permission required to record answers.");
      return;
    }
  }
  const audioOnly = new MediaStream(mediaStream.getAudioTracks());
  recordedChunks = [];
  mediaRecorder = new MediaRecorder(audioOnly);
  mediaRecorder.ondataavailable = (ev)=> {
    if(ev.data && ev.data.size>0) recordedChunks.push(ev.data);
  };
  mediaRecorder.onstop = () => {
    const blob = new Blob(recordedChunks, { type: 'audio/webm' });
    recordedChunks = [];
    const url = URL.createObjectURL(blob);
    const audio = document.createElement('audio');
    audio.controls = true;
    audio.src = url;
    const meta = document.createElement('div');
    meta.className = 'muted';
    meta.innerText = `Q${recordings.length+1}: ${ (new Date()).toLocaleTimeString() }`;
    const wrap = document.createElement('div');
    wrap.appendChild(audio);
    wrap.appendChild(meta);
    recordingsList.appendChild(wrap);
  };
}

recordBtn.onclick = async () => {
  if(isRecording) return;
  if(!mediaRecorder) await prepareRecorder();
  if(!mediaRecorder){
    return;
  }
  recordedChunks = [];
  mediaRecorder.start();
  isRecording = true;
  recordBtn.disabled = true;
  stopRec.disabled = false;
  transcriptEl.innerText = "listening...";
  // try SpeechRecognition
  try {
    const Rec = window.SpeechRecognition || window.webkitSpeechRecognition;
    if(Rec){
      recognition = new Rec();
      recognition.lang = 'en-US';
      recognition.interimResults = true;
      recognition.onresult = (ev) => {
        let txt = '';
        for(let i=0;i<ev.results.length;i++){
          txt += ev.results[i][0].transcript;
        }
        transcriptEl.innerText = txt;
      };
      recognition.start();
    }
  } catch(e){
    // ignore
  }
};

stopRec.onclick = () => {
  if(mediaRecorder && mediaRecorder.state === 'recording') mediaRecorder.stop();
  if(recognition){
    try { recognition.stop(); } catch(e){}
  }
  isRecording = false;
  recordBtn.disabled = false;
  stopRec.disabled = true;
  // store short metadata
  const t = transcriptEl.innerText || '';
  recordings.push({ q: questions[qIdx], transcript: t, time: Date.now() });
};

nextQ.onclick = () => {
  if(qIdx < questions.length - 1) {
    qIdx++;
    showQuestion(qIdx);
    transcriptEl.innerText = '‚Äî';
  } else {
    alert("This was the last question. Click Finish (when available) to see feedback.");
  }
};

// simple finish: compute heuristics client-side and POST to /feedback
function computeFeedback(){
  let totalWords = 0, fillers = 0;
  const fillerList = ['um','uh','like','you know','so','actually','basically'];
  recordings.forEach(r=>{
    const t = (r.transcript || "").toLowerCase();
    totalWords += (t.split(/\s+/).filter(Boolean)).length;
    fillerList.forEach(f => {
      const re = new RegExp('\\b'+f+'\\b','g');
      const m = t.match(re);
      if(m) fillers += m.length;
    });
  });
  const feedback = {
    questions_count: questions.length,
    recordings_count: recordings.length,
    total_words: totalWords,
    filler_count: fillers,
    posture_estimate: mediaStream ? 85 : 40
  };
  return feedback;
}

// when interview done (we add a finish button dynamically)
function showFinish(){
  endControls.innerHTML = '';
  const btn = document.createElement('button');
  btn.className = 'btn';
  btn.innerText = 'Finish & Get Feedback';
  btn.onclick = async () => {
    const fb = computeFeedback();
    await fetch('/feedback', {
      method: 'POST',
      headers: {'Content-Type':'application/json'},
      body: JSON.stringify(fb)
    });
    window.location.href = '/feedback';
  };
  endControls.appendChild(btn);
}

// when interview started enable finish after first answer recorded
const origStop = stopRec.onclick;
stopRec.addEventListener('click', ()=> {
  // whenever user stops a recording, show finish button if not shown
  showFinish();
});

</script>
</body>
</html>
